Lecture 2

Scraping
* Access HTML code in Chrome: F12
* Useful tags: heading <h1></h1> ... <h6></h6>, paragraph <p></p>, line break <br>,
link with attribute <a href="http://www.example.com/">An example link</a>
* Useful libraries in Python: urllib, beautifulsoup, pattern,soupy, LXML
* Python scraping tutorial: htts://github.com/kjam/python-web-scraping-tutorial

Lecture 3 - EDA (Really good lecture!)
* 1. Ask questions, 2. Get the data, 3. Explore the data, 4. Model the data, 5. Communicate and visualize the data
* If a plot doesn't start at zero, point it out.
* Don't use graphics that you don't need.
* Charts for comparison: Bar charts, line plots (bars for discrete time data),
  stacked bar charts (proportions).
* A pie chart is not as effective as a bar chart.
* Distribution plots: Histogram (play with bin size), density plot (continuous) .
* Design  (of visualizations) is an exercise of creating many alternatives quickly.
* Difference bar graph shows the difference between before and after.
* Sometimes raw numbers is best to communicate results.
* Good measures: length, position. Bad: color, shape, slope, angle, area, 
  and intensity.
* Quantitative data: Position (point), length (bar), slope (line), angle (pie).
* Ordered data: Area, Intensity
* Categories: Color, Shape
* Do not use more than 5-8 colors at once.
* Do not use rainbow color maps.
* Instead use greyscale color map with color to highlight data.
* For good color choices, go to Colorbrewer.

Lecture 4 - SQL, Relational Model/Grammar of Data
* Recommended database: Postgres, which is open source and supports both relational data and JSON
* A key should be unique, like an id. An integer is usually suitable.
* Nice table with translations: Verb, dplyr, pandas, SQL

Lecture 5 - Statistical models
* Exponential distribution, memoryless property: f(x) = lambda*exp(-lambda*x), x > 0
* Parametric distributions are connected in some ways.
* Poisson distribution for counts of rare events.
* 68% chance of being inside +-1 sigma if normally distributed
* 95% chance of being inside +-2 sigma if normally distributed
* 99% chance of being inside +-3 sigma if normally distributed
* It is often the case that if you have positive data and take the log, it becomes more normal/nicer to work with.
* Log-normal distribution: Becomes normally distributed if log is applied on it.

Lecture 6 - Communication and Storytelling (Great lecture on how to present your results well)
* Giving a speech, two obvious questions to answer: 1. What is the primary goals?, 2. Who cares?
* Choose and use notation carefully
* Include a simple, but memorable example, like Linda the bank teller
* Tell a story with a beginning, a middle, and an end. E.g. ants.
  * Introduce interesting characters
  * Put them in a predicament
  * Resolve the predicament
  * Leave room for sequels! That is: limitations and future work.
* If you give a talk, show a lot of visualizations
* Whenever you can tell a story with your data, do that!
* Who is your audience, what questions are you answering, why should the audience care, what are the major insights 
  and surprises.
* Know your audience:
  * What do they know?
  * What motivates them, what do they desire?
  * What experiences do you share, what are common goals?
  * What insights can you give them? What tools and "magical gifts"?
* Don't make your audience think too much. Make the main message very explicit!
* Use surprise to grab the audience's attention.
* Always keep your audience in mind. Know your audience.

Lecture 7 - Bias and Regression
* Selection bias, is introduced by the selection of individuals, groups or data for analysis in such a way that proper 
  randomization is not achieved
  * Always ask where data comes from. What data do you (not) have. 
* Publication bias. For example only publish papers with p<0.05, not showing non-significant studies
* The bias of an estimator is how far off it is on average: bias(theta_hat) = E(theta_hat) - theta
* MSE(theta_hat) = Var(theta_hat) + bias^2(theta_hat)
* Bonferroni correction, multiple testing
  * If multiple hypotheses are tested, the chance of a rare event increases, and therefore, the likelihood of incorrectly 
    rejecting a null hypothesis (i.e., making a Type I error) increases.
* In statistics, regression toward (or to) the mean is the phenomenon that if a variable is extreme on its first measurement, 
  it will tend to be closer to the average on its second measurement

Lecture 8 - Regression continued
* Odds = p/(1-p), where p is the probability
* Odds ratio is the ratio of the odds
* logit function: logit(p) = ln(p/(1-p)) = beta_0+beta_1*X_1+...+beta_k*X_k
* Curse of dimensionality: In high dimensions sometimes you have no choice but to do extrapolation, because you can't find
  enough nearby points.
* Blessing of dimensionality: More data! Can e.g. run hierarchical methods.

Lecture 9 - Classification, kNN, Dimensionality reduction
* A Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of 
  the plane. That set of points (called seeds, sites, or generators) is specified beforehand, and for each seed 
  there is a corresponding region consisting of all points closer to that seed than to any other. These regions 
  are called Voronoi cells. 
* Machine Learning methods are typically slow during training, but very fast during testing
  * With 1NN it is the other way around
* K-fold cross validation
  * Keep the test data out of the algorithm
    * Test data has to be completely new and unseen! 
    * That is: The test data is NOT used to determine parameters!
  * 5-fold if you have little data, otherwise 10-fold
* Dimensionality reduction can counter the curse of dimensions
* Multi-Dimensional Scaling
  * Find a set of points whose pairwise distances match a given distance matrix
  * Perform PCA on this set
  * Preserves the distances in the high-dimensional data in the projection.







